2022-01-23 18:37:01,297 INFO     Wrote config.txt file
2022-01-23 18:37:01,303 INFO     GerbilizerDenseNet(
  (pooling): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
  (f_convs): ModuleList(
    (0): Conv1d(4, 11, kernel_size=(19,), stride=(2,), padding=(9,))
    (1): Conv1d(15, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (2): Conv1d(25, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (3): Conv1d(35, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (4): Conv1d(45, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (5): Conv1d(55, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (6): Conv1d(65, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (7): Conv1d(75, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (8): Conv1d(85, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (9): Conv1d(95, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (10): Conv1d(105, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (11): Conv1d(115, 10, kernel_size=(19,), stride=(2,), padding=(9,))
  )
  (g_convs): ModuleList(
    (0): Conv1d(4, 11, kernel_size=(19,), stride=(2,), padding=(9,))
    (1): Conv1d(15, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (2): Conv1d(25, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (3): Conv1d(35, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (4): Conv1d(45, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (5): Conv1d(55, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (6): Conv1d(65, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (7): Conv1d(75, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (8): Conv1d(85, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (9): Conv1d(95, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (10): Conv1d(105, 10, kernel_size=(19,), stride=(2,), padding=(9,))
    (11): Conv1d(115, 10, kernel_size=(19,), stride=(2,), padding=(9,))
  )
  (norm_layers): ModuleList(
    (0): Identity()
    (1): Identity()
    (2): Identity()
    (3): Identity()
    (4): Identity()
    (5): Identity()
    (6): Identity()
    (7): Identity()
    (8): Identity()
    (9): Identity()
    (10): Identity()
    (11): Identity()
  )
  (x_coord_readout): Linear(in_features=125, out_features=3, bias=True)
  (y_coord_readout): Linear(in_features=125, out_features=3, bias=True)
)
2022-01-23 18:37:01,305 INFO     Training set:   <dataloaders.GerbilVocalizationDataset object at 0x141b94730>
2022-01-23 18:37:01,306 INFO     Validation set:   <dataloaders.GerbilVocalizationDataset object at 0x141b94850>
2022-01-23 18:37:01,306 INFO     Test set:   <dataloaders.GerbilVocalizationDataset object at 0x141b94910>
2022-01-23 18:37:01,306 INFO      ==== STARTING TRAINING ====

2022-01-23 18:37:01,306 INFO     >> SAVING INITIAL MODEL WEIGHTS TO /Users/alexanderwilliams/code/gerbilizer/trained_models/default/00003/init_weights.pt
2022-01-23 18:37:01,309 INFO     >> STARTING EPOCH 1
2022-01-23 18:37:07,764 INFO     TRAINING. 	 Epoch 1 / 10 [64/10521]   minibatch loss: 0.02002
2022-01-23 18:37:13,674 INFO     TRAINING. 	 Epoch 1 / 10 [128/10521]   minibatch loss: 0.01750
2022-01-23 18:37:19,590 INFO     TRAINING. 	 Epoch 1 / 10 [192/10521]   minibatch loss: 0.01427
2022-01-23 18:37:25,410 INFO     TRAINING. 	 Epoch 1 / 10 [256/10521]   minibatch loss: 0.01339
2022-01-23 18:37:32,004 INFO     TRAINING. 	 Epoch 1 / 10 [320/10521]   minibatch loss: 0.01347
2022-01-23 18:37:38,739 INFO     TRAINING. 	 Epoch 1 / 10 [384/10521]   minibatch loss: 0.01578
2022-01-23 18:37:44,573 INFO     TRAINING. 	 Epoch 1 / 10 [448/10521]   minibatch loss: 0.01590
2022-01-23 18:37:51,012 INFO     TRAINING. 	 Epoch 1 / 10 [512/10521]   minibatch loss: 0.01572
2022-01-23 18:37:57,065 INFO     TRAINING. 	 Epoch 1 / 10 [576/10521]   minibatch loss: 0.01464
2022-01-23 18:38:03,631 INFO     TRAINING. 	 Epoch 1 / 10 [640/10521]   minibatch loss: 0.01165
2022-01-23 18:38:09,468 INFO     TRAINING. 	 Epoch 1 / 10 [704/10521]   minibatch loss: 0.01610
2022-01-23 18:38:15,403 INFO     TRAINING. 	 Epoch 1 / 10 [768/10521]   minibatch loss: 0.01395
2022-01-23 18:38:21,185 INFO     TRAINING. 	 Epoch 1 / 10 [832/10521]   minibatch loss: 0.01342
2022-01-23 18:38:27,154 INFO     TRAINING. 	 Epoch 1 / 10 [896/10521]   minibatch loss: 0.01432
2022-01-23 18:38:32,920 INFO     TRAINING. 	 Epoch 1 / 10 [960/10521]   minibatch loss: 0.01487
2022-01-23 18:38:38,697 INFO     TRAINING. 	 Epoch 1 / 10 [1024/10521]   minibatch loss: 0.01398
2022-01-23 18:38:44,470 INFO     TRAINING. 	 Epoch 1 / 10 [1088/10521]   minibatch loss: 0.01409
2022-01-23 18:38:50,417 INFO     TRAINING. 	 Epoch 1 / 10 [1152/10521]   minibatch loss: 0.01317
2022-01-23 18:38:56,475 INFO     TRAINING. 	 Epoch 1 / 10 [1216/10521]   minibatch loss: 0.01269
2022-01-23 18:39:02,664 INFO     TRAINING. 	 Epoch 1 / 10 [1280/10521]   minibatch loss: 0.01448
2022-01-23 18:39:08,709 INFO     TRAINING. 	 Epoch 1 / 10 [1344/10521]   minibatch loss: 0.00957
2022-01-23 18:39:15,133 INFO     TRAINING. 	 Epoch 1 / 10 [1408/10521]   minibatch loss: 0.01662
2022-01-23 18:39:21,819 INFO     TRAINING. 	 Epoch 1 / 10 [1472/10521]   minibatch loss: 0.01420
2022-01-23 18:39:27,629 INFO     TRAINING. 	 Epoch 1 / 10 [1536/10521]   minibatch loss: 0.01275
2022-01-23 18:39:33,549 INFO     TRAINING. 	 Epoch 1 / 10 [1600/10521]   minibatch loss: 0.01641
2022-01-23 18:39:39,607 INFO     TRAINING. 	 Epoch 1 / 10 [1664/10521]   minibatch loss: 0.01296
2022-01-23 18:39:45,296 INFO     TRAINING. 	 Epoch 1 / 10 [1728/10521]   minibatch loss: 0.01348
2022-01-23 18:39:50,979 INFO     TRAINING. 	 Epoch 1 / 10 [1792/10521]   minibatch loss: 0.01440
2022-01-23 18:39:56,576 INFO     TRAINING. 	 Epoch 1 / 10 [1856/10521]   minibatch loss: 0.01644
2022-01-23 18:40:02,185 INFO     TRAINING. 	 Epoch 1 / 10 [1920/10521]   minibatch loss: 0.01384
2022-01-23 18:40:07,841 INFO     TRAINING. 	 Epoch 1 / 10 [1984/10521]   minibatch loss: 0.01359
2022-01-23 18:40:13,614 INFO     TRAINING. 	 Epoch 1 / 10 [2048/10521]   minibatch loss: 0.01435
2022-01-23 18:40:19,355 INFO     TRAINING. 	 Epoch 1 / 10 [2112/10521]   minibatch loss: 0.01183
2022-01-23 18:40:24,909 INFO     TRAINING. 	 Epoch 1 / 10 [2176/10521]   minibatch loss: 0.01313
2022-01-23 18:40:30,477 INFO     TRAINING. 	 Epoch 1 / 10 [2240/10521]   minibatch loss: 0.01121
2022-01-23 18:40:36,082 INFO     TRAINING. 	 Epoch 1 / 10 [2304/10521]   minibatch loss: 0.01318
2022-01-23 18:40:41,644 INFO     TRAINING. 	 Epoch 1 / 10 [2368/10521]   minibatch loss: 0.01329
2022-01-23 18:40:47,188 INFO     TRAINING. 	 Epoch 1 / 10 [2432/10521]   minibatch loss: 0.01425
2022-01-23 18:40:52,728 INFO     TRAINING. 	 Epoch 1 / 10 [2496/10521]   minibatch loss: 0.01304
2022-01-23 18:40:58,281 INFO     TRAINING. 	 Epoch 1 / 10 [2560/10521]   minibatch loss: 0.01438
2022-01-23 18:41:03,806 INFO     TRAINING. 	 Epoch 1 / 10 [2624/10521]   minibatch loss: 0.01398
2022-01-23 18:41:09,344 INFO     TRAINING. 	 Epoch 1 / 10 [2688/10521]   minibatch loss: 0.01336
2022-01-23 18:41:14,910 INFO     TRAINING. 	 Epoch 1 / 10 [2752/10521]   minibatch loss: 0.01731
2022-01-23 18:41:20,502 INFO     TRAINING. 	 Epoch 1 / 10 [2816/10521]   minibatch loss: 0.01297
2022-01-23 18:41:26,057 INFO     TRAINING. 	 Epoch 1 / 10 [2880/10521]   minibatch loss: 0.01479
2022-01-23 18:41:31,666 INFO     TRAINING. 	 Epoch 1 / 10 [2944/10521]   minibatch loss: 0.01372
2022-01-23 18:41:37,292 INFO     TRAINING. 	 Epoch 1 / 10 [3008/10521]   minibatch loss: 0.01483
2022-01-23 18:41:42,835 INFO     TRAINING. 	 Epoch 1 / 10 [3072/10521]   minibatch loss: 0.01251
2022-01-23 18:41:48,400 INFO     TRAINING. 	 Epoch 1 / 10 [3136/10521]   minibatch loss: 0.01353
2022-01-23 18:41:54,017 INFO     TRAINING. 	 Epoch 1 / 10 [3200/10521]   minibatch loss: 0.01543
2022-01-23 18:41:59,585 INFO     TRAINING. 	 Epoch 1 / 10 [3264/10521]   minibatch loss: 0.01346
2022-01-23 18:42:05,210 INFO     TRAINING. 	 Epoch 1 / 10 [3328/10521]   minibatch loss: 0.01477
2022-01-23 18:42:10,775 INFO     TRAINING. 	 Epoch 1 / 10 [3392/10521]   minibatch loss: 0.01461
2022-01-23 18:42:16,312 INFO     TRAINING. 	 Epoch 1 / 10 [3456/10521]   minibatch loss: 0.01211
2022-01-23 18:42:21,866 INFO     TRAINING. 	 Epoch 1 / 10 [3520/10521]   minibatch loss: 0.01256
2022-01-23 18:42:27,418 INFO     TRAINING. 	 Epoch 1 / 10 [3584/10521]   minibatch loss: 0.01239
2022-01-23 18:42:32,972 INFO     TRAINING. 	 Epoch 1 / 10 [3648/10521]   minibatch loss: 0.01253
2022-01-23 18:42:38,535 INFO     TRAINING. 	 Epoch 1 / 10 [3712/10521]   minibatch loss: 0.01325
2022-01-23 18:42:44,083 INFO     TRAINING. 	 Epoch 1 / 10 [3776/10521]   minibatch loss: 0.01362
2022-01-23 18:42:49,614 INFO     TRAINING. 	 Epoch 1 / 10 [3840/10521]   minibatch loss: 0.01290
2022-01-23 18:42:55,138 INFO     TRAINING. 	 Epoch 1 / 10 [3904/10521]   minibatch loss: 0.01512
2022-01-23 18:43:00,679 INFO     TRAINING. 	 Epoch 1 / 10 [3968/10521]   minibatch loss: 0.01382
2022-01-23 18:43:06,298 INFO     TRAINING. 	 Epoch 1 / 10 [4032/10521]   minibatch loss: 0.01185
2022-01-23 18:43:11,867 INFO     TRAINING. 	 Epoch 1 / 10 [4096/10521]   minibatch loss: 0.01173
2022-01-23 18:43:17,440 INFO     TRAINING. 	 Epoch 1 / 10 [4160/10521]   minibatch loss: 0.01390
2022-01-23 18:43:23,007 INFO     TRAINING. 	 Epoch 1 / 10 [4224/10521]   minibatch loss: 0.01251
2022-01-23 18:43:28,552 INFO     TRAINING. 	 Epoch 1 / 10 [4288/10521]   minibatch loss: 0.01538
2022-01-23 18:43:34,177 INFO     TRAINING. 	 Epoch 1 / 10 [4352/10521]   minibatch loss: 0.01336
2022-01-23 18:43:39,728 INFO     TRAINING. 	 Epoch 1 / 10 [4416/10521]   minibatch loss: 0.01046
2022-01-23 18:43:45,294 INFO     TRAINING. 	 Epoch 1 / 10 [4480/10521]   minibatch loss: 0.01258
2022-01-23 18:43:50,816 INFO     TRAINING. 	 Epoch 1 / 10 [4544/10521]   minibatch loss: 0.01057
2022-01-23 18:43:56,422 INFO     TRAINING. 	 Epoch 1 / 10 [4608/10521]   minibatch loss: 0.01340
2022-01-23 18:44:01,996 INFO     TRAINING. 	 Epoch 1 / 10 [4672/10521]   minibatch loss: 0.01521
2022-01-23 18:44:07,570 INFO     TRAINING. 	 Epoch 1 / 10 [4736/10521]   minibatch loss: 0.01249
2022-01-23 18:44:13,142 INFO     TRAINING. 	 Epoch 1 / 10 [4800/10521]   minibatch loss: 0.01563
2022-01-23 18:44:18,770 INFO     TRAINING. 	 Epoch 1 / 10 [4864/10521]   minibatch loss: 0.01346
2022-01-23 18:44:24,294 INFO     TRAINING. 	 Epoch 1 / 10 [4928/10521]   minibatch loss: 0.01341
2022-01-23 18:44:29,830 INFO     TRAINING. 	 Epoch 1 / 10 [4992/10521]   minibatch loss: 0.01277
2022-01-23 18:44:35,433 INFO     TRAINING. 	 Epoch 1 / 10 [5056/10521]   minibatch loss: 0.01342
2022-01-23 18:44:40,951 INFO     TRAINING. 	 Epoch 1 / 10 [5120/10521]   minibatch loss: 0.01113
2022-01-23 18:44:46,530 INFO     TRAINING. 	 Epoch 1 / 10 [5184/10521]   minibatch loss: 0.01538
2022-01-23 18:44:52,068 INFO     TRAINING. 	 Epoch 1 / 10 [5248/10521]   minibatch loss: 0.01357
2022-01-23 18:44:57,598 INFO     TRAINING. 	 Epoch 1 / 10 [5312/10521]   minibatch loss: 0.01441
2022-01-23 18:45:03,093 INFO     TRAINING. 	 Epoch 1 / 10 [5376/10521]   minibatch loss: 0.01224
2022-01-23 18:45:08,681 INFO     TRAINING. 	 Epoch 1 / 10 [5440/10521]   minibatch loss: 0.01414
2022-01-23 18:45:14,232 INFO     TRAINING. 	 Epoch 1 / 10 [5504/10521]   minibatch loss: 0.01013
2022-01-23 18:45:19,794 INFO     TRAINING. 	 Epoch 1 / 10 [5568/10521]   minibatch loss: 0.01089
2022-01-23 18:45:25,348 INFO     TRAINING. 	 Epoch 1 / 10 [5632/10521]   minibatch loss: 0.01384
2022-01-23 18:45:30,910 INFO     TRAINING. 	 Epoch 1 / 10 [5696/10521]   minibatch loss: 0.01259
2022-01-23 18:45:36,509 INFO     TRAINING. 	 Epoch 1 / 10 [5760/10521]   minibatch loss: 0.01227
2022-01-23 18:45:42,217 INFO     TRAINING. 	 Epoch 1 / 10 [5824/10521]   minibatch loss: 0.01301
2022-01-23 18:45:47,807 INFO     TRAINING. 	 Epoch 1 / 10 [5888/10521]   minibatch loss: 0.01451
2022-01-23 18:45:53,423 INFO     TRAINING. 	 Epoch 1 / 10 [5952/10521]   minibatch loss: 0.01316
2022-01-23 18:45:59,052 INFO     TRAINING. 	 Epoch 1 / 10 [6016/10521]   minibatch loss: 0.01338
2022-01-23 18:46:04,613 INFO     TRAINING. 	 Epoch 1 / 10 [6080/10521]   minibatch loss: 0.01556
2022-01-23 18:46:10,208 INFO     TRAINING. 	 Epoch 1 / 10 [6144/10521]   minibatch loss: 0.01171
2022-01-23 18:46:15,749 INFO     TRAINING. 	 Epoch 1 / 10 [6208/10521]   minibatch loss: 0.01204
2022-01-23 18:46:21,316 INFO     TRAINING. 	 Epoch 1 / 10 [6272/10521]   minibatch loss: 0.01200
2022-01-23 18:46:26,892 INFO     TRAINING. 	 Epoch 1 / 10 [6336/10521]   minibatch loss: 0.01342
2022-01-23 18:46:32,425 INFO     TRAINING. 	 Epoch 1 / 10 [6400/10521]   minibatch loss: 0.01494
2022-01-23 18:46:38,014 INFO     TRAINING. 	 Epoch 1 / 10 [6464/10521]   minibatch loss: 0.01458
2022-01-23 18:46:43,559 INFO     TRAINING. 	 Epoch 1 / 10 [6528/10521]   minibatch loss: 0.01281
2022-01-23 18:46:49,159 INFO     TRAINING. 	 Epoch 1 / 10 [6592/10521]   minibatch loss: 0.01150
2022-01-23 18:46:54,709 INFO     TRAINING. 	 Epoch 1 / 10 [6656/10521]   minibatch loss: 0.00972
2022-01-23 18:47:00,280 INFO     TRAINING. 	 Epoch 1 / 10 [6720/10521]   minibatch loss: 0.01108
2022-01-23 18:47:05,849 INFO     TRAINING. 	 Epoch 1 / 10 [6784/10521]   minibatch loss: 0.01407
2022-01-23 18:47:11,384 INFO     TRAINING. 	 Epoch 1 / 10 [6848/10521]   minibatch loss: 0.01512
2022-01-23 18:47:16,949 INFO     TRAINING. 	 Epoch 1 / 10 [6912/10521]   minibatch loss: 0.01137
2022-01-23 18:47:22,520 INFO     TRAINING. 	 Epoch 1 / 10 [6976/10521]   minibatch loss: 0.01327
2022-01-23 18:47:28,134 INFO     TRAINING. 	 Epoch 1 / 10 [7040/10521]   minibatch loss: 0.01389
2022-01-23 18:47:33,725 INFO     TRAINING. 	 Epoch 1 / 10 [7104/10521]   minibatch loss: 0.01060
2022-01-23 18:47:39,308 INFO     TRAINING. 	 Epoch 1 / 10 [7168/10521]   minibatch loss: 0.01212
2022-01-23 18:47:44,859 INFO     TRAINING. 	 Epoch 1 / 10 [7232/10521]   minibatch loss: 0.01217
2022-01-23 18:47:50,450 INFO     TRAINING. 	 Epoch 1 / 10 [7296/10521]   minibatch loss: 0.01153
2022-01-23 18:47:55,953 INFO     TRAINING. 	 Epoch 1 / 10 [7360/10521]   minibatch loss: 0.01308
2022-01-23 18:48:01,483 INFO     TRAINING. 	 Epoch 1 / 10 [7424/10521]   minibatch loss: 0.01082
2022-01-23 18:48:07,071 INFO     TRAINING. 	 Epoch 1 / 10 [7488/10521]   minibatch loss: 0.01351
2022-01-23 18:48:12,619 INFO     TRAINING. 	 Epoch 1 / 10 [7552/10521]   minibatch loss: 0.01100
2022-01-23 18:48:18,137 INFO     TRAINING. 	 Epoch 1 / 10 [7616/10521]   minibatch loss: 0.01271
2022-01-23 18:48:23,679 INFO     TRAINING. 	 Epoch 1 / 10 [7680/10521]   minibatch loss: 0.01246
2022-01-23 18:48:29,209 INFO     TRAINING. 	 Epoch 1 / 10 [7744/10521]   minibatch loss: 0.01184
2022-01-23 18:48:34,795 INFO     TRAINING. 	 Epoch 1 / 10 [7808/10521]   minibatch loss: 0.01097
2022-01-23 18:48:40,354 INFO     TRAINING. 	 Epoch 1 / 10 [7872/10521]   minibatch loss: 0.01028
2022-01-23 18:48:45,886 INFO     TRAINING. 	 Epoch 1 / 10 [7936/10521]   minibatch loss: 0.01307
2022-01-23 18:48:51,553 INFO     TRAINING. 	 Epoch 1 / 10 [8000/10521]   minibatch loss: 0.01015
2022-01-23 18:48:57,084 INFO     TRAINING. 	 Epoch 1 / 10 [8064/10521]   minibatch loss: 0.01235
2022-01-23 18:49:02,648 INFO     TRAINING. 	 Epoch 1 / 10 [8128/10521]   minibatch loss: 0.01327
2022-01-23 18:49:08,258 INFO     TRAINING. 	 Epoch 1 / 10 [8192/10521]   minibatch loss: 0.01213
2022-01-23 18:49:13,815 INFO     TRAINING. 	 Epoch 1 / 10 [8256/10521]   minibatch loss: 0.01160
2022-01-23 18:49:19,402 INFO     TRAINING. 	 Epoch 1 / 10 [8320/10521]   minibatch loss: 0.01473
2022-01-23 18:49:24,934 INFO     TRAINING. 	 Epoch 1 / 10 [8384/10521]   minibatch loss: 0.01032
2022-01-23 18:49:30,449 INFO     TRAINING. 	 Epoch 1 / 10 [8448/10521]   minibatch loss: 0.01259
2022-01-23 18:49:36,044 INFO     TRAINING. 	 Epoch 1 / 10 [8512/10521]   minibatch loss: 0.01223
2022-01-23 18:49:41,630 INFO     TRAINING. 	 Epoch 1 / 10 [8576/10521]   minibatch loss: 0.01200
2022-01-23 18:49:47,204 INFO     TRAINING. 	 Epoch 1 / 10 [8640/10521]   minibatch loss: 0.01282
2022-01-23 18:49:52,808 INFO     TRAINING. 	 Epoch 1 / 10 [8704/10521]   minibatch loss: 0.01182
2022-01-23 18:49:58,360 INFO     TRAINING. 	 Epoch 1 / 10 [8768/10521]   minibatch loss: 0.01357
2022-01-23 18:50:03,914 INFO     TRAINING. 	 Epoch 1 / 10 [8832/10521]   minibatch loss: 0.01016
2022-01-23 18:50:09,453 INFO     TRAINING. 	 Epoch 1 / 10 [8896/10521]   minibatch loss: 0.01169
2022-01-23 18:50:15,008 INFO     TRAINING. 	 Epoch 1 / 10 [8960/10521]   minibatch loss: 0.01156
2022-01-23 18:50:20,516 INFO     TRAINING. 	 Epoch 1 / 10 [9024/10521]   minibatch loss: 0.01011
2022-01-23 18:50:26,076 INFO     TRAINING. 	 Epoch 1 / 10 [9088/10521]   minibatch loss: 0.01095
2022-01-23 18:50:32,081 INFO     TRAINING. 	 Epoch 1 / 10 [9152/10521]   minibatch loss: 0.00881
2022-01-23 18:50:37,796 INFO     TRAINING. 	 Epoch 1 / 10 [9216/10521]   minibatch loss: 0.00919
2022-01-23 18:50:43,361 INFO     TRAINING. 	 Epoch 1 / 10 [9280/10521]   minibatch loss: 0.01047
2022-01-23 18:55:44,696 INFO     TRAINING. 	 Epoch 1 / 10 [9344/10521]   minibatch loss: 0.00938
2022-01-23 18:55:50,940 INFO     TRAINING. 	 Epoch 1 / 10 [9408/10521]   minibatch loss: 0.01320
2022-01-23 18:55:56,719 INFO     TRAINING. 	 Epoch 1 / 10 [9472/10521]   minibatch loss: 0.01108
2022-01-23 18:56:03,514 INFO     TRAINING. 	 Epoch 1 / 10 [9536/10521]   minibatch loss: 0.01107
2022-01-23 18:56:10,338 INFO     TRAINING. 	 Epoch 1 / 10 [9600/10521]   minibatch loss: 0.01131
2022-01-23 18:56:16,540 INFO     TRAINING. 	 Epoch 1 / 10 [9664/10521]   minibatch loss: 0.00914
2022-01-23 18:56:22,512 INFO     TRAINING. 	 Epoch 1 / 10 [9728/10521]   minibatch loss: 0.00911
2022-01-23 18:56:28,779 INFO     TRAINING. 	 Epoch 1 / 10 [9792/10521]   minibatch loss: 0.01117
2022-01-23 18:56:34,776 INFO     TRAINING. 	 Epoch 1 / 10 [9856/10521]   minibatch loss: 0.00807
2022-01-23 18:56:40,995 INFO     TRAINING. 	 Epoch 1 / 10 [9920/10521]   minibatch loss: 0.01139
2022-01-23 18:56:46,934 INFO     TRAINING. 	 Epoch 1 / 10 [9984/10521]   minibatch loss: 0.01107
2022-01-23 18:56:52,801 INFO     TRAINING. 	 Epoch 1 / 10 [10048/10521]   minibatch loss: 0.01262
2022-01-23 18:56:58,760 INFO     TRAINING. 	 Epoch 1 / 10 [10112/10521]   minibatch loss: 0.01172
2022-01-23 18:57:04,843 INFO     TRAINING. 	 Epoch 1 / 10 [10176/10521]   minibatch loss: 0.01000
2022-01-23 18:57:10,944 INFO     TRAINING. 	 Epoch 1 / 10 [10240/10521]   minibatch loss: 0.01165
2022-01-23 18:57:17,212 INFO     TRAINING. 	 Epoch 1 / 10 [10304/10521]   minibatch loss: 0.01111
2022-01-23 18:57:24,211 INFO     TRAINING. 	 Epoch 1 / 10 [10368/10521]   minibatch loss: 0.01049
2022-01-23 18:57:30,130 INFO     TRAINING. 	 Epoch 1 / 10 [10432/10521]   minibatch loss: 0.00859
2022-01-23 18:57:35,830 INFO     TRAINING. 	 Epoch 1 / 10 [10496/10521]   minibatch loss: 0.00970
2022-01-23 18:57:35,867 INFO     >> DONE TRAINING, STARTING TESTING.
2022-01-23 18:57:39,344 INFO     TESTING VALIDATION SET. Epoch 1 [128/1315]
2022-01-23 18:57:42,800 INFO     TESTING VALIDATION SET. Epoch 1 [256/1315]
2022-01-23 18:57:46,196 INFO     TESTING VALIDATION SET. Epoch 1 [384/1315]
2022-01-23 18:57:49,558 INFO     TESTING VALIDATION SET. Epoch 1 [512/1315]
2022-01-23 18:57:52,988 INFO     TESTING VALIDATION SET. Epoch 1 [640/1315]
2022-01-23 18:57:56,634 INFO     TESTING VALIDATION SET. Epoch 1 [768/1315]
2022-01-23 18:58:00,131 INFO     TESTING VALIDATION SET. Epoch 1 [896/1315]
2022-01-23 18:58:03,714 INFO     TESTING VALIDATION SET. Epoch 1 [1024/1315]
2022-01-23 18:58:07,221 INFO     TESTING VALIDATION SET. Epoch 1 [1152/1315]
2022-01-23 18:58:10,677 INFO     TESTING VALIDATION SET. Epoch 1 [1280/1315]
2022-01-23 18:58:11,686 INFO     >> FINISHED EPOCH IN: 21 mins, 10 secs
2022-01-23 18:58:11,687 INFO     >> VALIDATION LOSS IS BEST SO FAR, SAVING WEIGHTS TO /Users/alexanderwilliams/code/gerbilizer/trained_models/default/00003/best_weights.pt
2022-01-23 18:58:11,689 INFO     >> STARTING EPOCH 2
2022-01-23 18:58:11,689 INFO     >> TIME ELAPSED SO FAR:	21 mins, 10 secs
2022-01-23 18:58:11,690 INFO     >> EST. TIME REMAINING:	03 hours, 10 mins, 60 secs
2022-01-23 18:58:17,639 INFO     TRAINING. 	 Epoch 2 / 10 [64/10521]   minibatch loss: 0.00997
