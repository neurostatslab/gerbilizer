2022-01-23 21:44:49,758 INFO     Wrote config.txt file
2022-01-23 21:44:49,763 INFO     GerbilizerDenseNet(
  (pooling): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
  (f_convs): ModuleList(
    (0): Conv1d(4, 11, kernel_size=(15,), stride=(2,), padding=(7,))
    (1): Conv1d(15, 10, kernel_size=(15,), stride=(2,), padding=(7,))
    (2): Conv1d(25, 10, kernel_size=(15,), stride=(2,), padding=(7,))
    (3): Conv1d(35, 10, kernel_size=(15,), stride=(2,), padding=(7,))
    (4): Conv1d(45, 10, kernel_size=(15,), stride=(2,), padding=(7,))
    (5): Conv1d(55, 10, kernel_size=(15,), stride=(2,), padding=(7,))
    (6): Conv1d(65, 10, kernel_size=(13,), stride=(2,), padding=(6,))
    (7): Conv1d(75, 10, kernel_size=(11,), stride=(2,), padding=(5,))
    (8): Conv1d(85, 10, kernel_size=(9,), stride=(2,), padding=(4,))
    (9): Conv1d(95, 10, kernel_size=(7,), stride=(2,), padding=(3,))
    (10): Conv1d(105, 10, kernel_size=(5,), stride=(2,), padding=(2,))
    (11): Conv1d(115, 10, kernel_size=(3,), stride=(2,), padding=(1,))
  )
  (g_convs): ModuleList(
    (0): Conv1d(4, 11, kernel_size=(15,), stride=(2,), padding=(7,))
    (1): Conv1d(15, 10, kernel_size=(15,), stride=(2,), padding=(7,))
    (2): Conv1d(25, 10, kernel_size=(15,), stride=(2,), padding=(7,))
    (3): Conv1d(35, 10, kernel_size=(15,), stride=(2,), padding=(7,))
    (4): Conv1d(45, 10, kernel_size=(15,), stride=(2,), padding=(7,))
    (5): Conv1d(55, 10, kernel_size=(15,), stride=(2,), padding=(7,))
    (6): Conv1d(65, 10, kernel_size=(13,), stride=(2,), padding=(6,))
    (7): Conv1d(75, 10, kernel_size=(11,), stride=(2,), padding=(5,))
    (8): Conv1d(85, 10, kernel_size=(9,), stride=(2,), padding=(4,))
    (9): Conv1d(95, 10, kernel_size=(7,), stride=(2,), padding=(3,))
    (10): Conv1d(105, 10, kernel_size=(5,), stride=(2,), padding=(2,))
    (11): Conv1d(115, 10, kernel_size=(3,), stride=(2,), padding=(1,))
  )
  (norm_layers): ModuleList(
    (0): Identity()
    (1): Identity()
    (2): Identity()
    (3): Identity()
    (4): Identity()
    (5): Identity()
    (6): Identity()
    (7): Identity()
    (8): Identity()
    (9): Identity()
    (10): Identity()
    (11): Identity()
  )
  (x_coord_readout): Linear(in_features=125, out_features=3, bias=True)
  (y_coord_readout): Linear(in_features=125, out_features=3, bias=True)
)
2022-01-23 21:44:49,765 INFO     Training set:   <dataloaders.GerbilVocalizationDataset object at 0x145095790>
2022-01-23 21:44:49,765 INFO     Validation set:   <dataloaders.GerbilVocalizationDataset object at 0x1450958b0>
2022-01-23 21:44:49,765 INFO     Test set:   <dataloaders.GerbilVocalizationDataset object at 0x145095970>
2022-01-23 21:44:49,765 INFO      ==== STARTING TRAINING ====

2022-01-23 21:44:49,765 INFO     >> SAVING INITIAL MODEL WEIGHTS TO /Users/alexanderwilliams/code/gerbilizer/trained_models/default/00009/init_weights.pt
2022-01-23 21:44:49,767 INFO     >> STARTING EPOCH 1
2022-01-23 21:44:54,855 INFO     TRAINING. 	 Epoch 1 / 10 [64/10521]   minibatch loss: 0.02006
2022-01-23 21:44:59,474 INFO     TRAINING. 	 Epoch 1 / 10 [128/10521]   minibatch loss: 0.01618
2022-01-23 21:45:04,084 INFO     TRAINING. 	 Epoch 1 / 10 [192/10521]   minibatch loss: 0.01446
2022-01-23 21:45:08,681 INFO     TRAINING. 	 Epoch 1 / 10 [256/10521]   minibatch loss: 0.01484
2022-01-23 21:45:13,198 INFO     TRAINING. 	 Epoch 1 / 10 [320/10521]   minibatch loss: 0.01576
2022-01-23 21:45:17,723 INFO     TRAINING. 	 Epoch 1 / 10 [384/10521]   minibatch loss: 0.01461
2022-01-23 21:45:22,489 INFO     TRAINING. 	 Epoch 1 / 10 [448/10521]   minibatch loss: 0.01470
2022-01-23 21:45:27,073 INFO     TRAINING. 	 Epoch 1 / 10 [512/10521]   minibatch loss: 0.01269
2022-01-23 21:45:32,242 INFO     TRAINING. 	 Epoch 1 / 10 [576/10521]   minibatch loss: 0.01578
2022-01-23 21:45:37,020 INFO     TRAINING. 	 Epoch 1 / 10 [640/10521]   minibatch loss: 0.01228
2022-01-23 21:45:42,267 INFO     TRAINING. 	 Epoch 1 / 10 [704/10521]   minibatch loss: 0.01603
2022-01-23 21:45:47,031 INFO     TRAINING. 	 Epoch 1 / 10 [768/10521]   minibatch loss: 0.01450
2022-01-23 21:45:51,712 INFO     TRAINING. 	 Epoch 1 / 10 [832/10521]   minibatch loss: 0.01285
2022-01-23 21:45:56,200 INFO     TRAINING. 	 Epoch 1 / 10 [896/10521]   minibatch loss: 0.01283
2022-01-23 21:46:00,797 INFO     TRAINING. 	 Epoch 1 / 10 [960/10521]   minibatch loss: 0.01278
2022-01-23 21:46:05,615 INFO     TRAINING. 	 Epoch 1 / 10 [1024/10521]   minibatch loss: 0.01704
2022-01-23 21:46:10,333 INFO     TRAINING. 	 Epoch 1 / 10 [1088/10521]   minibatch loss: 0.01320
2022-01-23 21:46:15,044 INFO     TRAINING. 	 Epoch 1 / 10 [1152/10521]   minibatch loss: 0.01282
2022-01-23 21:46:19,639 INFO     TRAINING. 	 Epoch 1 / 10 [1216/10521]   minibatch loss: 0.01131
2022-01-23 21:46:24,288 INFO     TRAINING. 	 Epoch 1 / 10 [1280/10521]   minibatch loss: 0.01648
2022-01-23 21:46:29,626 INFO     TRAINING. 	 Epoch 1 / 10 [1344/10521]   minibatch loss: 0.01475
2022-01-23 21:46:34,448 INFO     TRAINING. 	 Epoch 1 / 10 [1408/10521]   minibatch loss: 0.01134
2022-01-23 21:46:39,166 INFO     TRAINING. 	 Epoch 1 / 10 [1472/10521]   minibatch loss: 0.01568
2022-01-23 21:46:43,989 INFO     TRAINING. 	 Epoch 1 / 10 [1536/10521]   minibatch loss: 0.01372
2022-01-23 21:46:48,694 INFO     TRAINING. 	 Epoch 1 / 10 [1600/10521]   minibatch loss: 0.01641
2022-01-23 21:46:53,497 INFO     TRAINING. 	 Epoch 1 / 10 [1664/10521]   minibatch loss: 0.01443
2022-01-23 21:46:58,361 INFO     TRAINING. 	 Epoch 1 / 10 [1728/10521]   minibatch loss: 0.01360
2022-01-23 21:47:03,244 INFO     TRAINING. 	 Epoch 1 / 10 [1792/10521]   minibatch loss: 0.01385
2022-01-23 21:47:07,992 INFO     TRAINING. 	 Epoch 1 / 10 [1856/10521]   minibatch loss: 0.01387
2022-01-23 21:47:12,804 INFO     TRAINING. 	 Epoch 1 / 10 [1920/10521]   minibatch loss: 0.01057
2022-01-23 21:47:17,542 INFO     TRAINING. 	 Epoch 1 / 10 [1984/10521]   minibatch loss: 0.01625
2022-01-23 21:47:22,264 INFO     TRAINING. 	 Epoch 1 / 10 [2048/10521]   minibatch loss: 0.01212
2022-01-23 21:47:27,030 INFO     TRAINING. 	 Epoch 1 / 10 [2112/10521]   minibatch loss: 0.01544
2022-01-23 21:47:31,720 INFO     TRAINING. 	 Epoch 1 / 10 [2176/10521]   minibatch loss: 0.01262
2022-01-23 21:47:36,519 INFO     TRAINING. 	 Epoch 1 / 10 [2240/10521]   minibatch loss: 0.01137
2022-01-23 21:47:41,113 INFO     TRAINING. 	 Epoch 1 / 10 [2304/10521]   minibatch loss: 0.01478
2022-01-23 21:47:45,996 INFO     TRAINING. 	 Epoch 1 / 10 [2368/10521]   minibatch loss: 0.01892
2022-01-23 21:47:50,642 INFO     TRAINING. 	 Epoch 1 / 10 [2432/10521]   minibatch loss: 0.01308
2022-01-23 21:47:55,387 INFO     TRAINING. 	 Epoch 1 / 10 [2496/10521]   minibatch loss: 0.01479
2022-01-23 21:48:00,219 INFO     TRAINING. 	 Epoch 1 / 10 [2560/10521]   minibatch loss: 0.01473
2022-01-23 21:48:05,050 INFO     TRAINING. 	 Epoch 1 / 10 [2624/10521]   minibatch loss: 0.01494
2022-01-23 21:48:09,877 INFO     TRAINING. 	 Epoch 1 / 10 [2688/10521]   minibatch loss: 0.01384
2022-01-23 21:48:14,368 INFO     TRAINING. 	 Epoch 1 / 10 [2752/10521]   minibatch loss: 0.01352
2022-01-23 21:48:18,974 INFO     TRAINING. 	 Epoch 1 / 10 [2816/10521]   minibatch loss: 0.01105
2022-01-23 21:48:23,880 INFO     TRAINING. 	 Epoch 1 / 10 [2880/10521]   minibatch loss: 0.01672
2022-01-23 21:48:28,609 INFO     TRAINING. 	 Epoch 1 / 10 [2944/10521]   minibatch loss: 0.01271
2022-01-23 21:48:33,467 INFO     TRAINING. 	 Epoch 1 / 10 [3008/10521]   minibatch loss: 0.01416
2022-01-23 21:48:38,209 INFO     TRAINING. 	 Epoch 1 / 10 [3072/10521]   minibatch loss: 0.01499
2022-01-23 21:48:42,892 INFO     TRAINING. 	 Epoch 1 / 10 [3136/10521]   minibatch loss: 0.01232
2022-01-23 21:48:47,634 INFO     TRAINING. 	 Epoch 1 / 10 [3200/10521]   minibatch loss: 0.01559
2022-01-23 21:48:52,537 INFO     TRAINING. 	 Epoch 1 / 10 [3264/10521]   minibatch loss: 0.01179
2022-01-23 21:48:57,297 INFO     TRAINING. 	 Epoch 1 / 10 [3328/10521]   minibatch loss: 0.01244
2022-01-23 21:49:02,044 INFO     TRAINING. 	 Epoch 1 / 10 [3392/10521]   minibatch loss: 0.01226
2022-01-23 21:49:06,604 INFO     TRAINING. 	 Epoch 1 / 10 [3456/10521]   minibatch loss: 0.01510
2022-01-23 21:49:11,743 INFO     TRAINING. 	 Epoch 1 / 10 [3520/10521]   minibatch loss: 0.01284
2022-01-23 21:49:17,000 INFO     TRAINING. 	 Epoch 1 / 10 [3584/10521]   minibatch loss: 0.01370
2022-01-23 21:49:21,993 INFO     TRAINING. 	 Epoch 1 / 10 [3648/10521]   minibatch loss: 0.01254
2022-01-23 21:49:27,274 INFO     TRAINING. 	 Epoch 1 / 10 [3712/10521]   minibatch loss: 0.01358
2022-01-23 21:49:31,869 INFO     TRAINING. 	 Epoch 1 / 10 [3776/10521]   minibatch loss: 0.01229
2022-01-23 21:49:37,162 INFO     TRAINING. 	 Epoch 1 / 10 [3840/10521]   minibatch loss: 0.01209
2022-01-23 21:49:41,949 INFO     TRAINING. 	 Epoch 1 / 10 [3904/10521]   minibatch loss: 0.01201
2022-01-23 21:49:46,797 INFO     TRAINING. 	 Epoch 1 / 10 [3968/10521]   minibatch loss: 0.01721
2022-01-23 21:49:51,939 INFO     TRAINING. 	 Epoch 1 / 10 [4032/10521]   minibatch loss: 0.01340
2022-01-23 21:49:56,502 INFO     TRAINING. 	 Epoch 1 / 10 [4096/10521]   minibatch loss: 0.01249
2022-01-23 21:50:01,482 INFO     TRAINING. 	 Epoch 1 / 10 [4160/10521]   minibatch loss: 0.01595
2022-01-23 21:50:06,434 INFO     TRAINING. 	 Epoch 1 / 10 [4224/10521]   minibatch loss: 0.01273
2022-01-23 21:50:11,206 INFO     TRAINING. 	 Epoch 1 / 10 [4288/10521]   minibatch loss: 0.01177
2022-01-23 21:50:16,269 INFO     TRAINING. 	 Epoch 1 / 10 [4352/10521]   minibatch loss: 0.01537
2022-01-23 21:50:20,915 INFO     TRAINING. 	 Epoch 1 / 10 [4416/10521]   minibatch loss: 0.01492
2022-01-23 21:50:26,111 INFO     TRAINING. 	 Epoch 1 / 10 [4480/10521]   minibatch loss: 0.01371
2022-01-23 21:50:31,235 INFO     TRAINING. 	 Epoch 1 / 10 [4544/10521]   minibatch loss: 0.01265
2022-01-23 21:50:36,191 INFO     TRAINING. 	 Epoch 1 / 10 [4608/10521]   minibatch loss: 0.01680
2022-01-23 21:50:41,190 INFO     TRAINING. 	 Epoch 1 / 10 [4672/10521]   minibatch loss: 0.01417
2022-01-23 21:50:46,558 INFO     TRAINING. 	 Epoch 1 / 10 [4736/10521]   minibatch loss: 0.01551
2022-01-23 21:50:51,737 INFO     TRAINING. 	 Epoch 1 / 10 [4800/10521]   minibatch loss: 0.01466
2022-01-23 21:50:56,669 INFO     TRAINING. 	 Epoch 1 / 10 [4864/10521]   minibatch loss: 0.01451
2022-01-23 21:51:01,922 INFO     TRAINING. 	 Epoch 1 / 10 [4928/10521]   minibatch loss: 0.01450
2022-01-23 21:51:07,064 INFO     TRAINING. 	 Epoch 1 / 10 [4992/10521]   minibatch loss: 0.01134
2022-01-23 21:51:12,436 INFO     TRAINING. 	 Epoch 1 / 10 [5056/10521]   minibatch loss: 0.01256
2022-01-23 21:51:17,653 INFO     TRAINING. 	 Epoch 1 / 10 [5120/10521]   minibatch loss: 0.01417
2022-01-23 21:51:22,772 INFO     TRAINING. 	 Epoch 1 / 10 [5184/10521]   minibatch loss: 0.01385
2022-01-23 21:51:28,259 INFO     TRAINING. 	 Epoch 1 / 10 [5248/10521]   minibatch loss: 0.01241
2022-01-23 21:51:33,591 INFO     TRAINING. 	 Epoch 1 / 10 [5312/10521]   minibatch loss: 0.01374
2022-01-23 21:51:38,371 INFO     TRAINING. 	 Epoch 1 / 10 [5376/10521]   minibatch loss: 0.01518
2022-01-23 21:51:43,049 INFO     TRAINING. 	 Epoch 1 / 10 [5440/10521]   minibatch loss: 0.01084
2022-01-23 21:51:47,602 INFO     TRAINING. 	 Epoch 1 / 10 [5504/10521]   minibatch loss: 0.01555
2022-01-23 21:51:52,353 INFO     TRAINING. 	 Epoch 1 / 10 [5568/10521]   minibatch loss: 0.01506
2022-01-23 21:51:57,325 INFO     TRAINING. 	 Epoch 1 / 10 [5632/10521]   minibatch loss: 0.01410
2022-01-23 21:52:03,199 INFO     TRAINING. 	 Epoch 1 / 10 [5696/10521]   minibatch loss: 0.01479
2022-01-23 21:52:09,965 INFO     TRAINING. 	 Epoch 1 / 10 [5760/10521]   minibatch loss: 0.01642
2022-01-23 21:52:15,910 INFO     TRAINING. 	 Epoch 1 / 10 [5824/10521]   minibatch loss: 0.01396
2022-01-23 21:52:21,111 INFO     TRAINING. 	 Epoch 1 / 10 [5888/10521]   minibatch loss: 0.01021
2022-01-23 21:52:26,656 INFO     TRAINING. 	 Epoch 1 / 10 [5952/10521]   minibatch loss: 0.01311
2022-01-23 21:52:31,479 INFO     TRAINING. 	 Epoch 1 / 10 [6016/10521]   minibatch loss: 0.01423
