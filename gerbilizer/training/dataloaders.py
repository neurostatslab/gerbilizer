"""
Functions to construct torch Dataset, DataLoader
objects and specify data augmentation.
"""

from itertools import combinations
from math import comb
import os
from typing import Optional, Tuple

import h5py
import numpy as np
from scipy.signal import correlate
import torch
from torch import randint
from torch.utils.data import Dataset, DataLoader


class GerbilVocalizationDataset(Dataset):
    def __init__(
        self,
        datapath,
        *,
        flip_vert: bool = False,
        flip_horiz: bool = False,
        segment_len: int = 256,
        arena_dims: Optional[Tuple[float, float]] = None,
        make_xcorrs: bool = False,
        inference: bool = False,
    ):
        """
        Args:
            datapath (str):
                Path to directory containing the 'snippet{idx}' subdirectories
            flip_vert (bool):
                When true, mirroring augmentation will be applied to data and labels
            flip_horiz (bool):
                When true, mirroring augmentation will be applied to data and labels
            segment_len (int):
                Length of the segments sampled from individual vocalizations within
                the dataset
            map_size (int):
                When provided, informs the dataloader of the size of the confidence maps
                generated by the model. When none, x and y coordinates will be provided
                as labels.
            arena_dims (float tuple):
                The width and length of the arena in which sounds are localized. Used to
                scale labels (in millimeters)
            make_xcorrs (bool):
                When true, the dataset will compute pairwise cross correlations between
                the traces of all provided microphones.
            inference (bool):
                When true, will not attempt to locate labels within dataset and will only
                return sounds
        """
        if isinstance(datapath, str):
            self.dataset = h5py.File(datapath, "r")
        else:
            self.dataset = datapath
        self.flip_vert = flip_vert
        self.flip_horiz = flip_horiz
        self.segment_len = segment_len
        self.samp_size = 1
        self.arena_dims = arena_dims
        self.make_xcorrs = make_xcorrs
        self.inference = inference
        self.n_channels = None

    def __del__(self):
        self.dataset.close()

    def __len__(self):
        if "len_idx" in self.dataset:
            return len(self.dataset["len_idx"]) - 1
        return len(self.dataset["vocalizations"])

    def _crop_audio(self, audio):
        # TODO: Delete this fn?
        pad_len = self.crop_audio
        n_samples = audio.shape[0]
        n_channels = audio.shape[1]

        new_len = min(pad_len, n_samples)
        clipped_audio = audio[:new_len, ...].T  # Change shape to (n_mics, <=pad_len)

        zeros = np.zeros((n_channels, pad_len), dtype=audio.dtype)
        zeros[:, : clipped_audio.shape[1]] = clipped_audio
        return zeros

    @classmethod
    def sample_segment(cls, audio, section_len):
        """Samples a contiguous segment of length `section_len` from audio sample `audio` randomly
        within margins extending 10% of the total audio length from either end of the audio sample.

        Returns: audio segment with shape (n_channels, section_len)
        """
        n_samp = len(audio)
        margin = int(n_samp * 0.1)
        idx_range = margin, n_samp - margin - section_len
        if n_samp - 2 * margin <= section_len:
            # If section_len is longer than the audio we're sampling from, randomly place the entire
            # audio sample within a block of zeros of length section_len
            padding = np.zeros((audio.shape[1], section_len))
            offset = randint(-margin, margin, (1,)).item()
            end = min(audio.shape[0] + offset, section_len)
            if offset < 0:
                padding[:, :end] = audio[-offset : end - offset, :].T
            else:
                padding[:, offset:end] = audio[: end - offset, :].T
            return padding
        start = randint(*idx_range, (1,)).item()
        end = start + section_len
        return audio[start:end, ...].T

    def _append_xcorr(self, audio, *, is_batch=False):
        # Assumes the audio has shape (n_channels, n_samples), which is true
        # after sample_segment has been called
        # Assumes unbatched input
        if is_batch:
            n_channels = audio.shape[1]
            audio_with_corr = np.empty(
                (audio.shape[0], n_channels + comb(n_channels, 2), audio.shape[2]),
                audio.dtype,
            )
        else:
            n_channels = audio.shape[0]
            audio_with_corr = np.empty(
                (n_channels + comb(n_channels, 2), audio.shape[1]), audio.dtype
            )
        self.n_channels = n_channels
        audio_with_corr[..., :n_channels, :] = audio

        if is_batch:
            for batch in range(audio.shape[0]):
                for n, (a, b) in enumerate(combinations(audio[batch], 2)):
                    # a and b are mic traces
                    corr = correlate(a, b, "same")
                    audio_with_corr[batch, n + n_channels, :] = corr
        else:
            for n, (a, b) in enumerate(combinations(audio, 2)):
                # a and b are mic traces
                corr = correlate(a, b, "same")
                audio_with_corr[n + n_channels, :] = corr

        return audio_with_corr

    def _audio_for_index(self, dataset, idx):
        """Gets an audio sample from the dataset. Will determine the format
        of the dataset and handle it appropriately.
        """
        if "len_idx" in dataset:
            start, end = dataset["len_idx"][idx : idx + 2]
            audio = dataset["vocalizations"][start:end, ...]
            if self.n_channels is None:
                self.n_channels = audio.shape[1]
            return audio
        else:
            return dataset["vocalizations"][idx]

    def _label_for_index(self, dataset, idx):
        return dataset["locations"][idx]

    @classmethod
    def scale_features(cls, inputs, labels, arena_dims, n_mics=4):
        """Scales the inputs to have zero mean and unit variance. Labels are scaled
        from millimeter units to an arbitrary unit with range [0, 1].
        """

        scaled_labels = None
        if labels is not None and arena_dims is not None:
            scaled_labels = np.empty_like(labels)

            # Shift range to [-1, 1]
            x_scale = arena_dims[0] / 2  # Arena half-width (mm)
            y_scale = arena_dims[1] / 2
            scaled_labels = labels / np.array([x_scale, y_scale])

        scaled_audio = np.empty_like(inputs)
        # std scaling: I think it's ok to use sample statistics instead of population statistics
        # because we treat each vocalization independantly of the others, their scale w.r.t other
        # vocalizations shouldn't affect our task
        raw_audio_mean = inputs[..., :n_mics, :].mean()
        raw_audio_std = inputs[..., :n_mics, :].std()
        scaled_audio[..., :n_mics, :] = (
            inputs[..., :n_mics, :] - raw_audio_mean
        ) / raw_audio_std
        if n_mics < inputs.shape[-2]:
            xcorr_mean = inputs[..., n_mics:, :].mean()
            xcorr_std = inputs[..., n_mics:, :].std()
            scaled_audio[..., n_mics:, :] = (
                inputs[..., 4:, :] - xcorr_mean
            ) / xcorr_std

        return scaled_audio, scaled_labels

    @staticmethod
    def unscale_features(labels, arena_dims):
        """Changes the units of `labels` from arb. scaled unit (in range [0, 1]) to
        centimeters.
        """
        x_scale = arena_dims[0] / 2
        y_scale = arena_dims[1] / 2
        scaled_labels = np.empty_like(labels)
        scaled_labels[..., 0] = labels[..., 0] * x_scale / 10
        scaled_labels[..., 1] = labels[..., 1] * y_scale / 10
        return scaled_labels

    def __getitem__(self, idx):

        sound = self._audio_for_index(self.dataset, idx)

        if self.samp_size > 1:
            sound = np.stack(
                [
                    self.sample_segment(sound, self.segment_len)
                    for _ in range(self.samp_size)
                ],
                axis=0,
            )
        else:
            sound = self.sample_segment(sound, self.segment_len)

        if self.make_xcorrs:
            sound = self._append_xcorr(sound, is_batch=self.samp_size > 1)

        # Load animal location in the environment.
        # shape: (2 (x/y coordinates), )
        location_map = (
            None if self.inference else self._label_for_index(self.dataset, idx)
        )

        arena_dims = (
            self.dataset["room_dims"][idx][:2] * 1000
            if "room_dims" in self.dataset
            else self.arena_dims
        )
        sound, location_map = GerbilVocalizationDataset.scale_features(
            sound,
            location_map,
            arena_dims=self.arena_dims,
            n_mics=self.n_channels,
        )

        if self.inference:
            return sound.astype("float32")

        return sound.astype("float32"), location_map.astype("float32")


def build_dataloaders(path_to_data, CONFIG):

    # Construct Dataset objects.
    train_path = os.path.join(path_to_data, "train_set.h5")
    val_path = os.path.join(path_to_data, "val_set.h5")
    test_path = os.path.join(path_to_data, "test_set.h5")

    if os.path.exists(train_path):
        traindata = GerbilVocalizationDataset(
            train_path,
            flip_vert=(CONFIG["AUGMENT_LABELS"] and CONFIG["AUGMENT_FLIP_VERT"]),
            flip_horiz=(CONFIG["AUGMENT_LABELS"] and CONFIG["AUGMENT_FLIP_HORIZ"]),
            segment_len=CONFIG["SAMPLE_LEN"],
            arena_dims=(CONFIG["ARENA_WIDTH"], CONFIG["ARENA_LENGTH"]),
            make_xcorrs=CONFIG["COMPUTE_XCORRS"],
        )
        train_dataloader = DataLoader(
            traindata, batch_size=CONFIG["TRAIN_BATCH_SIZE"], shuffle=True
        )
    else:
        train_dataloader = None

    if os.path.exists(val_path):
        valdata = GerbilVocalizationDataset(
            val_path,
            flip_vert=False,
            flip_horiz=False,
            segment_len=CONFIG["SAMPLE_LEN"],
            arena_dims=(CONFIG["ARENA_WIDTH"], CONFIG["ARENA_LENGTH"]),
            make_xcorrs=CONFIG["COMPUTE_XCORRS"],
        )
        val_dataloader = DataLoader(
            valdata, batch_size=CONFIG["VAL_BATCH_SIZE"], shuffle=False
        )
    else:
        val_dataloader = None

    if os.path.exists(test_path):
        testdata = GerbilVocalizationDataset(
            test_path,
            flip_vert=False,
            flip_horiz=False,
            segment_len=CONFIG["SAMPLE_LEN"],
            arena_dims=(CONFIG["ARENA_WIDTH"], CONFIG["ARENA_LENGTH"]),
            make_xcorrs=CONFIG["COMPUTE_XCORRS"],
        )
        test_dataloader = DataLoader(
            testdata, batch_size=CONFIG["TEST_BATCH_SIZE"], shuffle=False
        )
    else:
        test_dataloader = None

    return train_dataloader, val_dataloader, test_dataloader
